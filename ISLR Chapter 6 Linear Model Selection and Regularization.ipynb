{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Simple linear models can be improved by replacing plain least squares fitting with some alternative fitting procedures. Alternative fitting procedures can yield better prediction accuracy and model interpretability. \n",
    "* **Prediction Accuracy**: If n >> p, least squares estimate tends to have low variance and perform well on test data. However, if n is not much larger than p, there can be a lot of variability in the least squares fit resulting in overfitting. And if p > n, the variance is infinite and method cannot be used. By shrinking the estimated cofficients, we can substantially reduce variance at the cost of negligible increase in bias. \n",
    "* **Model Interpretability**: Irrelevant variables leads to unncessary complexity in the resulting model. By removing these varibales, we can obtain a model that is more easily interpreted. \n",
    "\n",
    "Alternatives to using least squared to fit:\n",
    "1. Subset Selection: Identify subset of p predictors that we believe to be related to the response. We fit the model using those reduced set of variables\n",
    "2. Shrinkage: Estimated coefficients are shrunken towards zero relative to the least squares estimates. The shrinkage has effect of reducing variance.\n",
    "3. Dimension Reduction: Projectng p predictors into M dimensional subspace where M < p. Those M projections are used as predictors to fit models by least squares. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset Selection\n",
    "### Best Subset Selection\n",
    "We fit seperate least squared regression for each possible combinations of the p predictors. Steps involved\n",
    "1. Null Model containing no predictors. Sample mean as the predictions (M0)\n",
    "2. For k = 1,2, ... p, fit models containing exactly k predictors. Pick the best model among all the models as $M_k$\n",
    "3. Select single best model from $M_0$, .....,$M_p$ using cross validation prediction erro, Cp, AIC, BIC or Adjusted Rsquared. \n",
    "\n",
    "### Stepwise Selection\n",
    "**Forward Stepwise Selection**\n",
    "Forward stepwise selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model. At each step the variable that gives the greatest additional improvement to the fit is added to the model. Though forward stepwise tends to do well in practice,it is not guaranteed to find the best possible model out of all $2^p$ models containing subsets of the p predictors. \n",
    "\n",
    "**Backward Stepwise Selection**\n",
    "Unlike forward stepwise selection, it begins with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-at-a-time. Backward selection requires that the number of samples n is larger than\n",
    "the number of variables p so that full model can be fit. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the optimal model\n",
    "Training error can be a poor estimate of the test error. Therefore, RSS and  $R^2$ are not suitable for selecting the best model among a collection of models with different numbers of predictors. Either we can indirectly estimate test error by making adjustment to training error by accounting for overfitting or directly estimate test error using Cv approach.\n",
    "\n",
    "$$ C_p = \\frac{1}{n} (RSS + 2d \\hat{\\sigma}^2) $$ where $\\hat{\\sigma}^2$ is estimate of variance of the error $\\epsilon$ asscoiated with each response measurement and is estimated using the full model containing all the predictors. \n",
    "\n",
    "The AIC criterion is defined for a large class of models fit by maximum likelihood. \n",
    "$$AIC = \\frac{1}{n\\hat{\\sigma}^2} (RSS + 2d \\hat{\\sigma}^2) $$\n",
    "\n",
    "BIC is derived from a Bayesian point of view, but ends up looking similar to Cp (and AIC) as well. For the least squares model with d predictors, theBIC is, up to irrelevant constants\n",
    "$$BIC = \\frac{1}{n\\hat{\\sigma}^2} (RSS + log(n)d \\hat{\\sigma}^2) $$\n",
    "\n",
    "The BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than Cp. \n",
    "\n",
    "The adjusted $R^2$ statistic is another popular approach for selecting among a set of models that contain different numbers of variables. \n",
    "\n",
    "$$ Adjusted R_2 = 1 - \\frac{RSS/(n-d-1)}{TSS/(n-1)} $$\n",
    "\n",
    "Therefore, in theory, the model with the largest adjusted $R^2$\n",
    "will have only correct variables and no noise variables. Unlike the $R^2$ statistic, the adjusted $R^2$ statistic pays a price for the inclusion of unnecessary variables in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrinkage Methods\n",
    "As an alternative, we can fit a model containing all p predictors using a technique that constrains or regularizes the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero.\n",
    "### Ridge Regression\n",
    "Ridge regression coefficients $\\beta_i$ are the values that minimize \n",
    "$$ \\sum\\limits_{i=1}^n \\Bigg(y_i - \\beta_0 - \\sum\\limits_{j=1}^p \\beta_j x_{ij}\\Bigg)^2 + \\lambda \\sum\\limits_{j=1}^{p} \\beta_j^2 $$ where $\\lambda$ is a tuning parameter also called regularization strength. When $\\lambda$ = 0, the penalty term has no effect, and ridge regression will produce the least squares estimates. However, as $\\lambda \\to\\infty$ the impact of the shrinkage penalty grows, and the ridge regression coefficient estimates will approach zero. Selecting a good value of $\\lambda$ is critical. The ridge regression coefficient estimates can change substantially when multiplying a given predictor by a constant. It is best to apply ridge regression after\n",
    "standardizing the predictors. Ridge regression’s advantage over least squares is rooted in the bias-variance trade-off. As $\\lambda $ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias.\n",
    "\n",
    "If p > n, then the least squares estimates do not even have a unique solution, whereas ridge regression can still perform well by trading off a small increase in bias for a large decrease in variance. Hence, ridge regression works best in situations\n",
    "where the least squares estimates have high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "Ridge regression will include all p predictors in the final model. The penality term will shrink all of the coefficients towards zero but will not set any of them exactly to zer0. Tt can create a challenge in model interpretation in settings in which the number of variables p is quite large. The lasso is a relatively recent alternative to ridge regression that over comes this disadvantage.\n",
    "\n",
    "$$ \\sum\\limits_{i=1}^n \\Bigg(y_i - \\beta_0 - \\sum\\limits_{j=1}^p \\beta_j x_{ij}\\Bigg)^2 + \\lambda \\sum\\limits_{j=1}^{p} |\\beta_j| $$\n",
    "\n",
    "$\\ell_1$ penalty has the effec of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter is sufficiently large. Hence, much like best subset selection, the lasso performs variable selection. Models generated from the lasso are generally much easier to interpret than those produced by ridge regression.\n",
    "\n",
    "Neither ridge regression nor the lasso will universally dominate the other. In general, one might expect the lasso to perform better in a setting where a relatively small number of predictors\n",
    "have substantial coefficients, and the remaining predictors have coefficien that are very small or that equal zero. Ridge regression will perform better when the response is a function of many predictors, all with coefficients of roughly equal size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction Methods\n",
    "We now explore a class of approaches that transform the predictors and then fit a least squares model using the transformed variables. We will refer to these tech niques as dimension reduction methods.\n",
    "### Principal Components Regression\n",
    "PCA is a technique for reducing the dimension of a nxp data matrix X. The first principal component direction of the data is that along which the observations vary the most. The second principal component Z2 is a linear combination of the variables that is uncorrelated with Z1,and has largest variance subject to this constraint.\n",
    "\n",
    "The principal components regression (PCR) approach involves constructing principal the firstM principal components,Z1, . . ., ZM, and then using these compo components regression nents as the predictors in a linear regression model that is fi using least squares. The key idea is that often a small number of principal components suffice to explain most of the variability in the data, as well as the relationship with the response. As more principal components are used in the regression model, the bias decreases, but the variance increases. PCR is more closely related to ridge regression than to the lasso. When performing PCR, we generally recommend standardizing each predictor, prior to generating the principal components. This standardization ensures that all variables are on the same scale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Least Squares\n",
    "There is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response. Unlike PCR, PLS identifies these new features in a supervised way—that is, it makes use of the response Y in order to identify new features that not only approximate the old features well, but also that are related to the response. PLS approach attempts to find directions that help explain both the response and the predictors. PLS places the highest weight on the variables that are most strongly related to the response. \n",
    "\n",
    " To identify the second PLS direction we first adjust each of the variables for Z1, by regressing each variable on Z1 and taking residuals. These resid uals can be interpreted as the remaining information that has not been explained by the first PLS direction. We then compute Z2 using this or thogonalized data in exactly the same fashion as Z1 was computed based on the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems in High Dimensions \n",
    " When the number of features p is as large as, or larger than, the number of observations n, least squares as described in cannot (or rather, should not ) be performed. Though it is possible to perfectly fi the training data in the high-dimensional setting, the resulting linear model will perform extremely poorly on an independent test set, and therefore does not constitute a useful model.\n",
    "\n",
    "(1) regularization or shrinkage plays a key role in high-dimensional problems, \n",
    "(2) appropriate tuning parameter selection is crucial for good predictive performance, and \n",
    "(3) the test error tends to increase as the dimensionality of the problem (i.e. the number of features or predictors) increases, unless the additional features are truly associated with the response. In the high-dimensional setting, the multicollinearity problem is extreme. \n",
    "\n",
    "We have seen that when p > n, it is easy to obtain a useless model that has zero residuals. It is important to instead report results on an independent test set, or cross-validation errors. For instance, the MSE or R2 on an independent test set is a valid measure of model fit, but the MSE on the training set certainly is not."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
