{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Resampling methods involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model.The process of evaluating a model’s performance is known as **model assessment**, whereas the process of selecting the proper level of flexibility for a model is known as **model selection**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set Approach\n",
    "It involves randomly dividing the available set of observations into two parts, a training set and a validation set or hold-out set. The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. The resulting validation hold-out set error rate provides the estimate of the test error rate. There are two drawbacks with this approach:\n",
    "1. The validation estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.\n",
    "2. In the validation approach, only a subset of the observations—those that are included in the training set rather than in the validation set—are used to fit the model. Since statistical methods tend to perform worse when trained on fewer observations, this suggests that the validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave One Out Cross Validation\n",
    "LOOCV involves splitting the set of observations into two parts. However, instead of creating two subsets of comparable size, a single observation (x1, y1) is used for the validation set, and the remaining observations {(x2, y2), . . , (xn,yn)} make up the training set. We repeat the process n times and LOOCV estimate for the test MSE ia the average of these n test error estimates\n",
    "$$ CV_{(n)} = \\frac{1}{n}\\sum\\limits_{i=1}^{n} MSE_i $$\n",
    "\n",
    "LOOCV approach tends not to overestimate the test error rate as much as the validation set approach does. In contrast to the validation approach which will yield different results when applied repeatedly due to randomness in the training/validation set splits, performing LOOCV multiple times will always yield the same results: there is no randomness in the training/validation set splits. LOOCV has the potential to be expensive to implement, since the model\n",
    "has to be fit n times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Fold Cross Validation\n",
    "\n",
    "An alternative to LOOCV is k-fold CV. This approach involves randomly dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k-1 folds. The mean squared error,MSE1,is\n",
    "then computed on the observations in the held-out fold. This procedure is repeated k times; each time, a different group of observations is treated as a validation set. This process results in k estimates of the test error,MSE1,MSE2, . .MSEk. The k-fold CV estimate is computed by averaging\n",
    "these values. \n",
    "\n",
    "$$ CV_{(k)} = \\frac{1}{k} \\sum\\limits_{i=1}^{k} MSE_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Variance Trade-Off for kFold Cross Validation\n",
    "\n",
    "Performing k-fold CV for, say, k = 5 or k = 10 will lead to an intermediate level of bias, since each training set contains (k-1)n/k observations—fewer than in the LOOCV approach, but substantially more than in the validation set\n",
    "approach. Therefore, from the perspective of bias reduction, it is clear that LOOCV is to be preferred to k-fold CV.\n",
    "\n",
    "It turns out that LOOCV has higher variance than does k-fold CV with k < n. When we perform LOOCV, we are in effect averaging the outputs of n fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated\n",
    "with each other. In contrast, when we perform k-fold CV with k < n,we are averaging the outputs of k fitted models that are somewhat less correlated with each other, since the overlap between the training sets in each model is smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bootstrap\n",
    "Bootstrap is a powerful statistical tool that can be used to quantify uncertainity associated with a given estimator or statistical learning method. \n",
    "\n",
    "The bootstrap approach allows us to use a computer to emulate the process of obtaining new sample sets. Rather than repeatedly obtaining independent data sets from the\n",
    "population, we instead obtain distinct data sets by repeatedly sampling observations from the original data set. The sampling is performed with replacement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
