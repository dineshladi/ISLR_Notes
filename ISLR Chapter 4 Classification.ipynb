{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An approach for predicting qualitative response is known as classification. \n",
    "\n",
    "For a binary response with a 0/1 coding as above, regression by least squares does make sense. if we use linear regression, some of our estimates might be outside the $[0, 1]$ interval, making them hard to interpret as probabilities!. Curiously, it turns out that the classifications that we get if we use linear regression to predict a binary response will be the same as for the linear discriminant analysis (LDA) procedure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Rather than modeling this response Y directly, logistic regression models the probability that Y belongs to a particular category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Model\n",
    "In logistic regression, we use the logistic function,\n",
    "\n",
    "$$ p(X) = p(Y = 1|X) =\\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}  $$\n",
    "\n",
    "To fit the model, we use maximum likelihood. The logistic function will always produce an S-shaped curve of this form, and so regardless of the value of X, we will obtain a sensible prediction.\n",
    "\n",
    "$$ \\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1 X} $$ \n",
    "\n",
    "The quantity $p(X)/(1-p(X))$ is called the odds, and can take on any value odds between 0 and Inf. Values of the odds close to 0 and Inf indicate very low and very high probabilities respectively. \n",
    "\n",
    "$$ log \\bigg(\\frac{p(X)}{1 - p(X)}\\bigg) = \\beta_0 + \\beta_1 X $$\n",
    "\n",
    "The left-hand side is called the log-odds or logit. We see that the logistic log-odds regression model has a logit that is linear in X. In contrast, in a logistic regression model, increasing X by one unit changes the log odds by $\\beta_1$ or equivalently it multiplies the odds by $e^{\\beta_1}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating coefficients\n",
    "\n",
    "We use a method called **maximum likelihood** to fit the model. \n",
    "$$ \\ell(\\beta_0,\\beta_1) = \\prod\\limits_{i:y_i=1} p(x_i) \\prod\\limits_{i:y_i=0} (1-p(x_i)) $$\n",
    "\n",
    "The estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are chosen to maximize this likelihood function. In the linear regression setting, the least squares approach is in fact a special case of maximum likelihood.\n",
    "\n",
    "The z-statistic in plays the same role as the t-statistic in the linear regression output.  A large (absolute) value of the z-statistic indicates evidence against the null hypothesis $H_0:\\beta_1 = 0$. \n",
    "\n",
    "$$ z = \\frac{\\hat{\\beta_1} - 0}{SE(\\hat{\\beta_1})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Logistic Regression\n",
    "\n",
    "$$ log \\bigg(\\frac{p(X)}{1 - p(X)}\\bigg) = \\beta_0 + \\beta_1 X_1 + ..... + \\beta_p X_p $$\n",
    "\n",
    "$$ p(X) =\\frac{e^{\\beta_0 + \\beta_1 X_1 + ..... + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + ..... + \\beta_p X_p}}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Logistic Regression\n",
    "The two-class logistic models discussed in the previous sections have multiple-class extensions, but in practice they tend not to be used all that often. We used LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis\n",
    "Logistic regression involves directly modeling $Pr(Y = k|X = x)$ using the logistic function for the case of two response classes. We model the conditional distribution of the response $Y$, given the predictor(s) $X$. In this alternative approach,\n",
    "we model the distribution of the predictors X separately in each of the response classes (i.e. given Y ), and then use Bayes’ theorem to flip these around into estimates for $Pr(Y =k|X = x)$. When these distributions are assumed to be normal, it turns out that the model is very similar in form to logistic regression.\n",
    "\n",
    "Why different approaches needed ?\n",
    "1. When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. \n",
    "2. If n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Bayes Theorem for Classification \n",
    "$$ p_k(X) =  Pr(Y=k|X=x) = \\frac{\\pi_k f_k(x)}{\\sum_{i=1}^{K}\\pi_i f_i(x)} $$ \n",
    "where $\\pi_k$ is the prior probability that a randomly chosen observation comes from the kth class, $f_k(x)$ denotes the density  function of $X$ for an observation that comes from kth class and $p_k(x)$ is the posterior probability that an observation posterior X = x belongs to the kth class. \n",
    "\n",
    "In general, estimating $p_k$ is easy if we have a random sample of Ys from the population: we simply compute the fraction of the training observations that belong to the kth class. However, estimating $f_k(x)$ tends to be more challenging, unless we assume some simple forms for these densities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA for p = 1\n",
    "For now, assume that p = 1—that is, we have only one predictor. In order to estimate $f_k(x)$, we will first make some assumptions about its form. Suppose we assume that $f_k(x)$ is normal or Gaussian.In the onedimensional setting, the normal density takes the form \n",
    "\n",
    "$$ f_k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k} \\exp{ \\bigg[-\\frac{1}{2\\sigma_{k}^2}(x-\\mu_k)^2 \\bigg] } $$\n",
    "\n",
    "If we assume that there is shared variance across all K classes, plugging in the $f_k(x)$ in the probability equation and maximizing the same by applying log, we arrive here \n",
    "\n",
    "$$\\delta_k(x) = x . \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + log(\\pi_k) $$  \n",
    "\n",
    "For instance, if K = 2 and $\\pi_1 = \\pi_2$, then Bayes Classifier assigns an observation to class 1 if $2x(\\mu_1-\\mu_2) > \\mu_1^2 - \\mu_2^2$ and to class 2 otherwise. \n",
    "\n",
    "The word linear in the classifier’s name stems from the fact that the discriminant functions $\\delta_k(x)$ are linear functions of x as opposed to a more complex function of x.\n",
    "\n",
    "LDA methods approximate the Bayes classifier by plugging estimates for $\\pi_k$, $\\mu_k$ and $\\sigma^2$. \n",
    "\n",
    "$$ \\hat{\\mu_k} = \\frac{1}{n_k} \\sum\\limits_{i:y_i=k} x_i $$\n",
    "\n",
    "$$ \\hat{\\sigma}^2 = \\frac{1}{n - K} \\sum\\limits_{k=1}^{K} \\sum\\limits_{i:y_i=k} \\big(x_i - \\hat{\\mu_k}\\big)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA for p > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extend the LDA classifier to the case of multiple predictors. To do this, we will assume that X =(X1,X2,. . .Xp) is drawn from a multi variate Gaussian (or multivariate normal) distribution, with a class-specific multivariate mean vector and a common covariance matrix. The multivariate Gaussian distribution assumes that each individual predictor follows a one-dimensional normal distribution with some correlation between each pair of predictors.\n",
    "\n",
    "$$f(x) = \\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}} exp \\Big(-\\frac{1}{2} (x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\Big) $$\n",
    "\n",
    "To indicate that a p-dimensional random variable X has a multivariateGaussian distribution, we write $X \\sim N(\\mu,S)$. Here $E(X) = \\mu$ is the mean of X (a vector with p components), and $Cov(X) = \\Sigma$ is the pxp covariance matrix of X.\n",
    "\n",
    "Bayes classifier assigns an observation X = x to the class for which\n",
    "\n",
    "$$\\delta_k(x) = x^T \\Sigma^{-1}\\mu_k - \\frac{1}{2} \\mu_k^{T}\\Sigma^{-1}\\mu_k + log(\\pi_k) $$\n",
    "is the largest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadractic Discriminant Analysis\n",
    "Like LDA, the QDA classifier results from assuming that the observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes’ theorem in order to perform prediction. However, unlike LDA, QDA assumes that each class has its own covariance matrix. \n",
    "\n",
    "$$\\delta_k(x) = -\\frac{1}{2}(x-\\mu_k)^T \\Sigma_k^{-1} (x-\\mu_k) - \\frac{1}{2}|\\Sigma_k| + log(\\pi_k) $$\n",
    "$$ \\delta_k(x) = - \\frac{1}{2} x^T\\Sigma_{k}^{-1} x + x^T\\Sigma_k^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma_{k}^{-1} \\mu_k - \\frac{1}{2}|\\Sigma_k| + log(\\pi_k) $$\n",
    "\n",
    "**Why would one prefer LDA to QDA, or vice-versa?** The answer lies in the bias-variance trade-off.When there are p predictors, then estimating a covariance matrix requires estimating p(p+1)/2 parameters. QDA estimates a separate covariance matrix for each class, for a total of Kp(p+1)/2 parameters. Consequently, LDA is a much less flexible classifier than QDA, and\n",
    "so has substantially lower variance. If LDA’s assumption that the K classes share a common covariance matrix is badly off, then LDA can suffer from high bias. QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix for the K classes is clearly untenable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Classficiation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both logistic regression and LDA produce linear decision boundaries. The only differenc between the two approaches lies in the fact that ß0 and ß1 are estimated using maximum likelihood, whereas c0 and c1 are computed using the estimated mean and variance from a normal distribution. LDA assumes that the observations are drawn from a Gaussian distribution with a common covariance matrix in each class, and so can provide some improvements over logistic regression when this assumption approximately holds. Conversely, logistic regression can outperform LDA if these Gaussian assumptions are not met.\n",
    "\n",
    "KNN is a completely non-parametric approach:no assumptions are made about the shape of the decision boundary. We can expect this approach to dominate LDA and logistic regression when the decision boundary is highly non-linear.  QDA serves as a compromise between the non-parametric KNN method and the linear LDA and logistic regression approaches. Since QDA assumes a quadratic decision boundary, it can accurately model a wider range of problems than can the linear methods. Though not as flexible as KNN, QDA can perform better in the presence of a limited number of training observations because it does make some assumptions about the form of the decision boundary.\n",
    "\n",
    "When the true decision boundaries are linear, then the LDA and logistic regression approaches will tend to perform well.When\n",
    "the boundaries are moderately non-linear, QDA may give better results. Finally, for much more complicated decision boundaries, a non-parametric approach such as KNN can be superior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
