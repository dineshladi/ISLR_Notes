{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Support vector machine is a generalization of a simple and intuitive classifier called the maximal margin classifier. Support vector machines are intended for the binary classification setting in which there are two classes. \n",
    "\n",
    "## Maximal Margin Classifier\n",
    "### What is hyperplane ?\n",
    "In a p-dimensional space, a hyperplane is a flat affine subspace of dimension p-1. A hyperplane is defined by the equation \n",
    "$$\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ..... + \\beta_p X_p = 0 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximal Margin Classifier\n",
    "If data can be perfectly seperated using a hyperplane, then there will in fact exist an infinite number of such hyperplanes. A natural choice is the maximal margin hyperplane (also known as the maximal optimal separating hyperplane), which is the separating hyperplane that is farthest from the training observations. We hope that a classifier that has a large maximal margin on the training data will also have a large margin on the test data, and hence will classify the test observations correctly. It can lead to overftting when p is large. The maximal margin hyperplane represents the mid-line of the widest “slab” that we can insert between the two classes.\n",
    "\n",
    "The observation that support the maximal margin hyperplane are called **support vectors**. The maximal margin hyperplane depends directly on the support vectors, but not on the other observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of Maximal Margin Classifier\n",
    "Maximal margin hyperplane is the solution to the optimization problem. Maximize $M$ \n",
    "subject to  $$ \\sum\\limits_{j=1}^p \\beta_j^2 = 1 $$\n",
    "$$ yi(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + .... + \\beta_p x_{p1}) >= M $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier \n",
    "If no seperating hyperplane exits then there is no maximal margin classifier. In this case, optimization problem has no solution with $M > 0$. We can develop a hyperplane that almost separates the classes, using a so-called **soft margin**. The generalization of the maximal margin classifier to the non-separable case is known as the support vector classifier. \n",
    "\n",
    "Rather than seeking the largest possible margin so that every observation is not only on the correct side of the hyperplane but also on the correct side of the margin, we instead allow some observations to be on the incorrect side of the margin, or even the incorrect side of the hyperplane. \n",
    "\n",
    "### Detail fo Support Vector Classifier \n",
    "It is a solution to the optimization problem that maximize $M$ subject to \n",
    "\n",
    "subject to  $$ \\sum\\limits_{j=1}^p \\beta_j^2 = 1 $$\n",
    "$$ yi(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + .... + \\beta_p x_{p1}) >= M(1-\\epsilon_i) $$\n",
    "$$ \\epsilon_i >= 0, \\sum\\limits_{i=1}^n \\epsilon_i <= C $$ where C is a non negative tuning parameter. \n",
    "\n",
    "If $\\epsilon_i = 0$, then the ith observation is on the correct side of the margin. If $\\epsilon_i > 0$ then the ith observation is on the wrong side of the margin, and we say that the ith observation has violated the margin. If $\\epsilon_i > 0$ then it is on the wrong side of the hyperplane.\n",
    "\n",
    "C bounds the sum of the $\\epsilon_i$’s, and so it determines the number and severity of the violations to the margin (and to the hyperplane) that we will tolerate. As the budget C increases, we become more tolerant of violations to the margin, and so the margin will widen. Conversely, as C decreases, we become less tolerant of violations to the margin and so the margin narrows.\n",
    "\n",
    "Observations that lie directly on the margin, or on the wrong side of the margin for their class, are known as **support vectors**. These observations do affect the support vector classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "A general mechanism for converting a linear classifier into ones tha produce non-linear decision boundaries. \n",
    "\n",
    "### Classification with Non-linear decision boundaries\n",
    "The performance of linear regression can suffer when there is a non linear relationship between the predictors and the outcome. In the case of support vector classifier, we could address the problem of possibly non-linear boundaries between classes in a similar way, by enlarging the feature space using quadratic, cubic, and even higher-order polynomial functions of the predictors. There are many possible ways to enlarge the feature space, and that unless we are careful, we could end up with a huge number of features. Then computations would become unmanageable.\n",
    "\n",
    "### The Support Vector Machine\n",
    "The support vector machine (SVM) is an extension of the support vector classifier that results from enlarging the feature space in a specific way, using kernels to accomodate a non-linear decision boundary between the classes. Kernel approach is computationaly efficient. \n",
    "\n",
    "$$ f(x) = \\beta_0 + \\sum\\limits_{i \\in S} \\alpha_i K(x,x_i) $$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship to Logistic Regression\n",
    "\n",
    "$$ L(X,y,\\beta) = \\sum\\limits_{i=1}^{n} max[0,1-y_i(\\beta_0 + \\beta_1 x_{i1} + ... + \\beta_p x_{ip})] $$ \n",
    "This is known as hinge loss. Hinge loss is closely related to the loss function used in logistic regression. \n",
    "\n",
    "An interesting characteristic of the support vector classifier is that only support vectors play a role in the classifier obtained; observations on the correct side of the margin do not affect it. This is due to fact that loss function is exactly zero for observations for wich $y_i(\\beta_0 + \\beta_1 x_{i1} + .... \\beta+p x_{ip}) >= 1; these correspond to observations that are on the correct side of the margin. Loss for logistic regression is not exactly zero anywhere. But it is verysmall for observations that are far from the decision boundary.\n",
    "When the classes are well separated, SVMs tend to behave better than logistic regression; in more overlapping regimes, logistic regression is often preferred.\n",
    "\n",
    "The use of non-linear kernels is much more widespread in the context of SVMs than in the context of logistic regression or other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
