{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Tree based methods involve stratifying or segmenting the predictor space into a number of simple regions. Since the set of splitting rules used to segment the predictor space can be summarized in a tree, these types of approaches are known as decision tree methods. Tree-based methods are simple and useful for interpretation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "Decision trees can be applied to both regression and classification problems. \n",
    "\n",
    "Decision tree consists of a series of splitting rules starting at the top of the tree. Unfortunately, it is computationally infeasible to consider every possible partition of the feature space into J boxes. For this reason, we take a top-down, greedy approach that is known as **recursive binary splitting**. \n",
    "\n",
    "It is greedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step. \n",
    "\n",
    "We select the predictor $X_j$ and cutpoint $s$ such that splitting the predictor space into regions leads to greatest possible reduction in RSS. We repeat the process for the previously identified regions until a stopping criterion is reached. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Pruning\n",
    "The process described above may produce good predictions on the training set, but is likely to overfit the data, leading to poor test set performance. This is because the resulting tree might be too complex. A smaller tree with fewer splits (that is, fewer regions R1,. . .RJ) might lead to lower variance and better interpretation at the cost of a little bias. Grow a very large tree and then prune it back in order to obtain a subtree. Our goal is to select a subtree that leads to the lowest test error rate. But estimating CV error for every possible subtree can be cumbersome. \n",
    "\n",
    "Cost complexity pruningâ€”also known as weakest link pruning. Rather than considering every possible subtree, we consider a sequence of trees indexed by a nonnegative tuning parameter $\\alpha$. Use kCV to choose $\\alpha$. \n",
    "\n",
    "For each $\\alpha$ there coreesponds a subtree T in $T_0$ such that \n",
    "$$ \\sum\\limits_{m=1}^{|T|} \\sum\\limits_{x \\in R_m} (y_i - \\hat{y}_{R_m})^2 + \\alpha |T| $$ \n",
    "\n",
    "where $|T|$ is number of terminal nodes and $R_m$ is the rectangle corresponding to mth terminal node and $\\hat{y}_{R_m}$ is predicted response associated with $R_m$. As $\\alpha$ increases, there is a price to pay for having a tree with many terminal nodes, and so the quantity will tend to be minimized for a smaller subtree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Trees\n",
    "A classification tree is very similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one.\n",
    "\n",
    "It turns out classification error is not sufficiently sensitive for tree growing and in practice we use Gini and Entropy. \n",
    "\n",
    "Gini Index is defined by \n",
    "\n",
    "$$ G = \\sum\\limits_{k=1}^K \\hat{p}_{mk} (1-\\hat{p}_{mk}) $$ is measure of variance across K classes. Gini index is referred to as measure of purity. An alternative to the Gini index is **entropy** give by \n",
    "$$ D = - \\sum\\limits_{k=1}^K \\hat{p}_{mk} log(\\hat{p}_{mk}) $$\n",
    "\n",
    "When building a classification tree, either the Gini index or the entropy are typically used to evaluate the quality of a particular split, since these two approaches are more sensitive to node purity than is the classification error rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree vs Linear Models \n",
    "If the relationship between the features and the response is well approximated by a linear model, then an approach such as linear regression will likely work well, and will outperform a method such as a regression tree that does not exploit this linear structure. \n",
    "\n",
    "### Pros and Cons of Tress\n",
    "Pros:\n",
    "1. Easy to explain\n",
    "2. Decision trees more closely mirror human decision making \n",
    "3. Can be easily interpreted \n",
    "4. Easily handle qualitative predictors \n",
    "\n",
    "Cons:\n",
    "1. Ttrees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches.\n",
    "2. Trees can be very non-robust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging, Random Forests and Boosting\n",
    "\n",
    "### Bagging\n",
    "Decisions trees suffer from high variance. Bootstrap aggregation, or bagging, is a general-purpose procedure for reducing the variance of a statistical learning method. Averaging a set of observations reduces variance. We can bootstrap, by taking repeated\n",
    "samples from the (single) training data set and train our models on bootstrapped training set and finally average all the predictions. Each individual tree has high variance, but low bias. Averaging these B trees reduces the variance. \n",
    "\n",
    "#### Out of Bag Error Estimation\n",
    "On an average each bagged tree makes use of around two thirds of the observations. The remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations. We can predict the response for the ith observation using each of the trees in which that observation was OOB. An OOB prediction can be obtained in this way for each of the n observations, from which the overall OOB MSE (for a regression problem) or classification error (for a classification problem) can be computed. The resulting OOB error is a valid estimate of the test error for the bagged model, since the response for each observation is predicted using only the trees that were not fit using that observation. \n",
    "\n",
    "#### Variable Importance\n",
    "Bagging improves prediction accuracy at the expense of interpretability. One can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees). We recod total amount that RSS is decreased due to splits over a given predictor averaged across all B trees. A large value indicates an important predictor. Similarly, in the context of bagging classificatio trees, we can add up the total amount that the Gini index is decreased by splits over a given predictor, averaged over all B trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests \n",
    "Random forests provide an improvement over bagged trees by way of a small tweak that **decorrelates** the trees. When building these decision trees, each time a split in a tree is considered, a random sample of m predictions is chosen as split condiates from full set of $p$ predictors. If there is a strong predictor in a dataset, in a collection of bagged DTs, most of the trees will use the strong predictor in the top split. So, all of the bagged trees will look quite similar. Hence, predictions from the bagged trees will be highly correlated. Averaging highly correlated quantities doesnt lead to a large reduction in variance as averaging many uncorrelated quantities. \n",
    "\n",
    "Random forests overcome this problem by forcing each split to consider only a subset of the predictors. We can think of this process as decorrelating the trees, thereby making the average of the resulting trees less variable and hence more reliable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "Like bagging, boosting is a general\n",
    "approach that can be applied to many statistical learning methods for regression or classification.\n",
    "In Boosting, trees are grown sequentially. Each tree is grown using information from previously\n",
    "grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.\n",
    "\n",
    "Unlike fitting a single large deci\n",
    "sion tree to the data, which amounts to fitting the data hard and potentially overfitting, the boosting approach instead **learns slowly**. Given the current model, we fit a decision tree to the residuals from the model. That is, we fit a tree using the current residuals, rather than the outcome Y, as the response. We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter d in the algorithm. The shrinkage parameter $\\lambda$ slows the process down even further, allowing more and different shaped trees to attack the residuals.\n",
    "\n",
    "Tuning parameters in boosting:\n",
    "1. Number of trees. \n",
    "2. Shrinkage paramter $\\lambda$. This controls the rate at which boosting learns. \n",
    "3. The number of splits in each tree, which controls the complexity of the boosted ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
