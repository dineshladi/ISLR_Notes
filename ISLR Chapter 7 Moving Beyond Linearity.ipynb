{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Standard linear regression can have significant limitations in terms of predictive power since linearity asumption is almost always an approximation. Simple extensions of linear models like polynomial regression, step functions, splines, local regression and generalized additive models. \n",
    "* **Polynomial regression** extends the linear model by adding extra predictors, obtained by raising each of the original predictors to a power.\n",
    "* *Step functions* cut the range of a variable into K distinct regions in order to produce a qualitative variable. \n",
    "* **Regression Splines** They involve dividing the range of X into K distinct regions. Within each region, a polynomial function is fit to the data. However, these polynomials are constrained so that they join smoothly at the region boundaries, or knots.\n",
    "* **Smoothing Splines** Smoothing splines result from minimizing a residual sum of squares criterion subject to a smoothness penalty.\n",
    "* **Generalized additive models** allow us to extend the methods above to deal with multiple predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Functions\n",
    "We break the range of X into bins, and fit a different constant in each bin.This amounts to converting a continuous variable into an ordered categorical variable.\n",
    "\n",
    "### Basis Functions\n",
    "Polynomial and piecewise-constant regression models are in fact special cases of a basis function approach. \n",
    "\n",
    "$$ y_i = \\beta_0 + \\beta_1 b_1(x_i) + \\beta_2 b_2(x_i) + .... \\beta_K b_K(x_i) + \\epsilon_i $$\n",
    "\n",
    "And these basis functions are fixed and known. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Splines\n",
    "#### Piecewise Polynomials\n",
    "Instead of fitting a high-degree polynomial over the entire range of X, piecewise polynomial regression involves fitting separate low-degree polynomials  over different regions of X. In gen\n",
    "eral, if we place K different knots throughout the range of X, then we will end up fitting K + 1 different cubic polynomials. \n",
    "\n",
    "#### Constraints and Splines\n",
    "Different constraints like piecewise polynomials should be continuous, smooth etch. Each constraint that we impose on the piecewise cubic polynomials effectively frees up one degree of freedom, by reducing the complexity of the resulting piecewise polynomial fit. A cubic spline with K knots uses a total of 4+K degrees of freedom. \n",
    "\n",
    "#### Choosing number and location of knots\n",
    "The regression spline is most flexible in regions that contain a lot of knots, because in those regions the polynomial coefficients can change rapidly. One option is to place more knots in places where we feel the function might vary most rapidly, and to place fewer knots where it seems more stable. \n",
    "\n",
    "How many knots should we use, or equivalently how many degrees offreedom should our spline contain? One option is to try out different numbers of knots and see which produces the best looking curve.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing Splines\n",
    "While fitting a smoothed curve to a set of data, we want to find some function g(x) that makes RSS small and also smooth. You can find a function g(x) that minimizes \n",
    "\n",
    "$$ \\sum\\limits_{i=1}^n \\big(y-g(x_i)\\big)^2 + \\lambda \\int g''(t)^2 dt $$ where $\\lambda$ is non negative tuning parameter. \n",
    "\n",
    "The second derivative of a function is a measure of its roughness: it is large in absolute value if g(t) is very wiggly near t, and it is close to zero otherwise. The larger the value of $\\lambda$, the smoother g will be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Regression\n",
    "\n",
    "Local regression is a different approach for fitting flexible non-linear functions, which involves computing the fit at a target point x0 using only the nearby training observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Additive Models\n",
    "\n",
    "Generalized additive models (GAMs) provide a general framework for  extending a standard linear model by allowing non-linear functions of each of the variables, while maintaining additivity. \n",
    "\n",
    "$$ y_i = \\beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + ...... f_p(x_{ip}) + \\epsilon_i $$ \n",
    "\n",
    "Basic building blocks of GAMs can be smoothing splines\n",
    "\n",
    "Pros and Cons of GAMs:\n",
    "1. No need to manually try out many different transformations on each variable individually.\n",
    "2. Non-linear fits can potentially make more accurate predictions for the response Y\n",
    "3. Because the model is additive, we can still examine the effect of each $X_j$ on Y individually while holding all of other variables fixed. \n",
    "4. Limitations of GAMs is that the model is restricted to be additive. With many variables, import interactions can be missed. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
