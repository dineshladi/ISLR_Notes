{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Learning refers to set of tools for understanding data. These tools can be classified as supervised or unsupervised. **Supervised** learning involves building statistical models for predicting an ouput from one or more inputs. With **unsupervised** there is no output but we can learn structure and relationships from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input variable is denoted by symbol $X$ also called independent variables/predictors/features etc. Output variable is denoted by symbol $Y$ also called dependent variable or response variable.\n",
    "$$ Y = f(X) + \\epsilon $$\n",
    "where $f$ is fixed but unknown function of $X_1$, $X_2$ .... $X_p$ and $\\epsilon$ is random error term which is independent of X and has mean zero. One must estimate $f$ based on observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E(Y-\\hat{y})^2 = E[f(X) + \\epsilon - \\hat{f}(X)]^2 \n",
    "                 = [f(X) - \\hat{f}(X)]^2 + Var(\\epsilon) \n",
    "                 = Var(\\hat{f}(X)) + Bias(\\hat{f}(X))^2 + Var(\\epsilon)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "Simple Linear regression is an approach for predicting a quantitative response variable $Y$ on the basis of a single predictor variable $X$. It assumes that there is a linear relationship between $X$ and $Y$ \n",
    "\n",
    "$$ Y \\approx \\beta_0 + \\beta_1 X $$\n",
    "\n",
    "$\\beta_0$ and $\\beta_1$ are unknown constants that represent intercept and slope terms in the linear model. Together, the 2 terms are called coefficients or parameters. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Coefficients\n",
    "\n",
    "$$ \\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1} X $$\n",
    "$$ Y = f(X) + \\epsilon $$\n",
    "\n",
    "$$ RSS = \\sum\\limits_{n=1}^{N} \\left(Y - \\hat{Y}\\right)^2 $$\n",
    "\n",
    "Using least squares approach chooses $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ to minimize the RSS, we get \n",
    "\n",
    "$$\\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right) \\left(y_i - \\bar{y}\\right)}{\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)^2} $$\n",
    "\n",
    "$$\\hat{\\beta_0} = \\bar{y} - \\beta_1 \\bar{x} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assesing Accuracy\n",
    "$$ Y = \\beta_0 + \\beta_1 X + \\epsilon $$\n",
    "\n",
    "$$ Var(\\hat{\\mu}) = SE\\left(\\hat{\\mu}\\right)^2 = \\frac{\\sigma^2}{n} $$\n",
    "\n",
    "$$ SE(\\hat{\\beta_0})^2 = \\sigma^2 \\bigg[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^{n}\\left(x_i - \\bar{x}\\right)^2} \\bigg]$$\n",
    "\n",
    "$$ SE(\\hat{\\beta_1})^2 = \\frac{\\sigma^2}{\\sum_{i=1}^{N}\\left(x_i - \\bar{x}\\right)^2} $$\n",
    "\n",
    "For linear regression, the 95% confidence interval for $\\beta_1$ approximately takes the form\n",
    "$$ \\big[\\hat{\\beta}_1 - 2 . SE(\\hat{\\beta}_1), \\hat{\\beta}_1 + 2 . SE(\\hat{\\beta}_1) \\big] $$\n",
    "\n",
    "t statistic is given by \n",
    "$$ t = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)} $$\n",
    "\n",
    "\n",
    "measures the number of standard deviations that $\\beta_1$ is away from 0.\n",
    "\n",
    "We reject the null hypothesis—that is, we declare a relationship to exist between $X$ and $Y$ if the p-value is small enough\n",
    "\n",
    "Estimate of $\\sigma$ is known as residual standard error and given by formula \n",
    "\n",
    "$$RSE = \\sqrt{\\frac{1}{n-2} RSS} = \\sqrt{\\frac{1}{n-2} \\sum\\limits_{i=1}^{n} (y_i - \\bar{y})^2} $$\n",
    "\n",
    "RSE is considered a measure of the lack of fit of the model to the data.\n",
    "\n",
    "$R^2$:The statistic provides an alternative measure of fit. It takes the form of a proportion—the proportion of variance\n",
    "explained—and so it always takes on a value between 0 and 1, and is independent of the scale of $Y$.\n",
    "\n",
    "$$ R^2 = \\frac{TSS - RSS}{TSS}  = 1 - \\frac{RSS}{TSS}$$\n",
    "\n",
    "TSS measures the total variance in the response $Y$, and can be squares thought of as the amount of variability inherent in the response before the regression is performed. RSS measures the amount of variability that is left unexplained after performing the regression. R2 measures the proportion of variability in $Y$ that can be explained using $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 ....... \\beta_p X_p + \\epsilon $$ \n",
    "\n",
    "F Statistic \n",
    "\n",
    "$$F = \\frac{(TSS − RSS)/p}{RSS/(n − p − 1)} $$\n",
    "\n",
    "$$RSE = \\sqrt{\\frac{1}{n-p-1} RSS} $$\n",
    "\n",
    "$$ R_{adj}^{2} = 1 -  \\frac{RSS/(n-k-1)}{TSS/(n-1)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Problems of Linear Regression\n",
    "1. **Non-linearity of the response-predictor relationships**: If the true relationship is far from linear, then virtually all of the conclusions that we draw from the fit are suspect. The prediction accuracy can be significantly reduced. Residual plots are useful tools for identifying non-linearity. The presence of a pattern may indicate a problem with some aspect of the linear model.\n",
    "2. **Correlation of error terms**: An important assumption of linear regression is that error terms are uncorrelated. If in fact there is correlation among the error terms, then the estimated standard errors will tend to underestimate the true standard errors. As a result, confidence and prediction intervals will be narrower than they should be. In short, if the error terms are correlated, we may have an unwarranted sense of confidence in our model. Why might correlations among the error terms occur? Such correlations frequently occur in the context of time series data, which consists of observations for which measurements are obtained at discrete points in time. In many cases, observations that are obtained at adjacent time points will have positively correlated errors.\n",
    "3. **Non-constant Variance of Error Terms**: Another important assumption of linear regression is error terms have constant variance. The standard errors, confidence intervals, and hypothesis tests associated with the linear model rely upon this assumption. One can identify non-constant variances inthe errors, or heteroscedasticity, from the presence of a funnel shape in the residual plot. \n",
    "4. **Outliers**: Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection. It is typical for an outlier that does not have an unusual predictor value to have little effect on the least squares fit. However, even if an outlier does not have much effect on the least squares fit, it can cause other problems. Since the RSE is used to compute all confidence intervals and p-values, such a dramatic increase caused by a single data point can haveimplications for the interpretation of the fit.Residual plots can be used to identify outliers. If we believe that an outlier has occurred due to an error in data collection or recording, then one solution is to simply remove the observation. However, care should be taken, since an outlier may instead indicate a deficiency with the model, such as a missing predictor. \n",
    "5. **Hight Leverage Points**: observations with high leverage high leverage have an unusual value for $x_i$. High leverage observations tend to have a sizable impact on the estimated regression line. In a simple linear regression, high leverage observations are fairly easy to identify, since we can simply look for observations for which the predictor value is outside of the normal range of the observations. But in a multiple linear regression with many predictors, it is possible to have an observation that is well within the range of each individual predictor’s values, but that is unusual in terms of the full set of predictors. \n",
    "6. **Collinearity**: Collinearity refers to the situation in which two or more predictor variables are closely related to one another. The presence of collinearity can pose problems in the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response. There will be broad range of values for the coefficient estimates that result in equal value for RSS. small change in the data could cause the pair of coefficient values that yield the smallest RSS—that is, the least squares estimates—to move anywhere along this valley. This results in a great deal of uncertainty in the coefficient estimates. Since collinearity reduces the accuracy of the estimates of the regression coefficients, it causes the standard error for $\\hat{\\beta_i}$ to grow. Consequently, collinearity results in a decline in the t-statistic. As a result, in the presence of collinearity, we may fail to reject null hypothesis $H_0: \\beta_j = 0$. This means that the power of the hypothesis test—the probability of correctly power detecting a non-zero coefficient—is reduced by collinearity. A simple way to detect collinearity is to look at the correlation matrixof the predictors. An element of this matrix that is large in absolute value indicates a pair of highly correlated variables, and therefore a collinearity problem in the data. Unfortunately, not all collinearity problems can be detected by inspection of the correlation matrix: it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation. We call this situation multicollinearity. Instead of inspecting the correlation matrix, a better way to assess multicollinearity is to compute the variance inflation factor (VIF). The VIF is the ratio of the variance of $\\hat{\\beta_i}$ when fitting the full model divided by the ratio of variance of $\\hat{\\beta_i}$ if fit on its own. The smallest possible value for VIF is 1,which indicates the complete absence of collinearity. Typically in practice there is a small amount of collinearity among the predictors. As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity. The VIF for each variable can be computed using the formula. When faced with the problem of collinearity, there are two simple solutions. The first is to drop one of the problematic variables from the regression.This can usually be done without much compromise to the regression fit, since the presence of collinearity implies that the information that this variable provides about the response is redundant in the presence of the other variables. The second solution is to combine the collinear variables together into a single predictor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
