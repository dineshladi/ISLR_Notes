{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The goal is to discover interesting things about the measurements onX1,X2,. . .Xp. Unsupervised learning is often much more challenging. The exercise tends to be more subjective, and there is no simple goal for the analysis, such as prediction of a response. \n",
    "\n",
    "## Principal Component Analysis\n",
    "When faced with a large set of correlated variables, principal components allow us to summarize this set with a smaller number of representative variables that collectively explain most of the variability in the original set. Principal component analysis (PCA) refers to the process by which prin cipal components are computed, and the subsequent use of these components in understanding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Principal Components\n",
    "A better method is required to\n",
    "visualize the n observations when p is large. In particular, we would like to find a low-dimensional representation of the data that captures as much of the information as possible. PCA finds a low-dimensional representation of a data set that contains as much as possible of the variation. Each of the dimensions found by PCA is a linear combination of the p features. \n",
    "\n",
    "The first principal component of a set of features X1,X2, . . , Xp is the normalized linear combination of the features that has the highest variance\n",
    "\n",
    "$$ Z_1 = \\phi_{11}X_1 + \\phi_{21}X_2 + ..... + \\phi_{p1}X_p $$ and \n",
    "$$ \\sum\\limits_{j=1}^p \\phi_{j1}^2 = 1 $$ \n",
    "The elements $\\phi_j$ are the loadings of the first principal component. The loadings are solved via eigen decomposition. The second principal component is the linear combination of X1,. . . , Xp that has maximal variance out of all linear combinations that are uncorrelated with Z1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Interpretation of Principal Components\n",
    "Principal component loading vectors as the directions in feature space along which the data vary the most, and the principal component scores as projections along these directions. \n",
    "\n",
    "An alternative interpretation for principal components can also be useful: principal components provide low-dimensional linear surfaces that are closest to the observations.The first principal component loading vector has a very special property: it is the line in p-dimensional space that is closest to the n observations. The first two principal components of a data set span the plane that is closest to the n observations, in terms of average squared Euclidean distance. \n",
    "\n",
    "$$ x_{ij} \\approx \\sum\\limits_{m=1}^M z_{im} \\phi_{jm} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on PCA\n",
    "* The results obtained when we perform PCA will also depend on whether the variables have been individually scaled. Because it is undesirable for the principal components obtained to depend on an arbitrary choice of scaling, we typically scale each variable to have standard deviation one before we perform PCA.\n",
    "* Each principal component loading vector is unique. \n",
    "* How much of variance in the data is explained by the principal components. We are interested in proportion of variance explained (PVE) by each principal component. The PVE of the mth principal component is given by \n",
    "\n",
    "$$ \\frac{\\sum\\limits_{i=1}^{n}\\Big(\\sum\\limits_{j=1}^{p} \\phi_{jm} x_{ij}\\Big)^2}{\\sum\\limits_{j=1}^{p} \\sum\\limits_{i=1}^{n} x_{ij}^2} $$\n",
    "\n",
    "* We would like to use the smallest number of principal components required to get a good understanding of the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Methods\n",
    "Clustering refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. We seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in differen groups are quite different from each other. \n",
    "\n",
    "### K-Means Clustering\n",
    "K-means clustering is a simple and elegant approach for partitioning a data set into K distinct, non-overlapping clusters. To perform K-means clustering, we must first specify the desired number of clusters K; then the K-means algorithm will assign each observation to exactly one of the K\n",
    "clusters. \n",
    "\n",
    "The idea behind K-means clustering is that a good clustering is one for which the within-cluster variation is as small as possible. The within-cluster variation for cluster $C_k$ is a measure W(Ck) of the amount by which the observations within a cluster differ from each other. Objective is to\n",
    "\n",
    "$$ \\min_{C_1,C_2...,C_K}^{}  {\\sum\\limits_{k=1}^{K} W(C_k)} $$\n",
    "\n",
    "$$ W(C_k) = \\frac{1}{|C_k|} \\sum\\limits_{i,i' \\in C_k} \\sum\\limits_{j=1}^{p}\\big(x_{ij} - x_{i'j}\\big)^2 $$\n",
    "\n",
    "Algorithm:\n",
    "1. Radomly assign cluster numbers from 1 to K to each of the observations. \n",
    "2. Repeat\n",
    "\t1. For each cluster, compute centroid.\n",
    "\t2. Assign each observations to the closest centroid. \n",
    "\n",
    "Because the K-means algorithm finds a local rather than a global optimum, the results obtained will depend on the initial (random) cluster as signment of each observation. It is important to run the algorithm multiple times from different random initial configurations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering\n",
    "One potential disadvantage of K-means clustering is that it requires us to pre-specify the number of clusters K. Hierarchical clustering is an alternative approach which does not require that we commit to a particular choice of K. Hierarchical clustering has an added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations, called a dendrogram.\n",
    "\n",
    "\n",
    "Agglomerative or bottom-up clustering is most common type of hierarchial clustering refers to the fact that dendrogram is built starting from leaves and combining clusters up to the trunk. \n",
    "\n",
    "#### Interpreting Dendrogram\n",
    "For any two observations, we can look for the point in the tree where branches containing those two observations are first fused. The height of this fusion, as measured on the vertical axis, indicates how different the two observations are. Thus, observations that fuse at the very bottom of the tree are quite similar to each other, whereas observations that fuse close to the top of the tree will tend to be quite different. In order to identify clusters on the basis of dendrogram, we make a horizontal cut across the dendrogram. The distinct sets of observations beneath the cut can be interpreted as clusters \n",
    "\n",
    "#### The Hierarchical Clustering Algorithm\n",
    "The algorithm proceeds iteratively. Starting out at the bottom of the dendrogram, each of the n observations is treated as its own cluster. The two clusters that are most similar to each other are then fused so that there now are n-1 clusters. Next the two clusters that are most similar to each other are fused again, so that there now are n-2 clusters. The algorithm proceeds in this fashion until all of the observations belong to one single cluster, and the dendrogram is complete.\n",
    "\n",
    "The concept of dissimilarity between a pair of observations needs to be extended to a pair of groups of observations. This extension is achieved by developing the notion of linkage, which defines the dissimilarity between two groups of observations. The four most common types of linkageâ€”complete, average, single,\n",
    "and centroid.\n",
    "\n",
    "* Complete Linkage: Maximal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these\n",
    "dissimilarities.\n",
    "* Single: Minimal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities.\n",
    "* Average: Mean intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities.\n",
    "\n",
    "Generally preferred linkage is average and complete since they tend to yield balanced dendrograms. \n",
    "\n",
    "#### Choice of Dissimilarity Measure \n",
    "The choice of dissimilarity measure is very important, as it has a strong effect on the resulting dendrogram. In addition to carefully selecting the dissimilarity measure used, one must also consider whether or not the variables should be scaled to have standard deviation one before the dissimilarity between the observations is computed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues in Clustering\n",
    "In addition, clustering methods generally are not very robust to perturbations to the data. For instance, suppose that we cluster n observations, and then cluster the observations again after removing a subset of the n observations at random. One would hope that the two sets of clusters obtained would be quite similar, but often this is not the case.\n",
    "\n",
    "Performing clustering with different choices of parameters is recommended and looking at the full set of results in order to see what patterns consistently emerge. Since clustering can be non-robust, we recommend clustering subsets of the data in order to get a sense of the robustness of the clusters obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
